{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e58787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataanalysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataanalysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Protein Data Analysis')\n",
    "\n",
    "# Upload CSV file\n",
    "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(uploaded_file)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    st.subheader('Data Overview')\n",
    "    st.write(data.info())\n",
    "    st.write(data.head())\n",
    "\n",
    "    # Drop the 'MouseID' column\n",
    "    data = data.drop('MouseID', axis=1)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    data['Genotype'] = data['Genotype'].map({'Control': 0, 'Ts65Dn': 1})\n",
    "    data['Treatment'] = data['Treatment'].map({'Saline': 0, 'Memantine': 1})\n",
    "    data['Behavior'] = data['Behavior'].map({'C/S': 0, 'S/C': 1})\n",
    "\n",
    "    # Label encode the 'class' column\n",
    "    encode = LabelEncoder().fit(data['class'])\n",
    "    data['class'] = encode.transform(data['class'])\n",
    "\n",
    "    # Save the encoder using pickle\n",
    "    with open('enc.pickle', 'wb') as f:\n",
    "        pickle.dump(encode, f)\n",
    "\n",
    "    # Handle missing values\n",
    "    missing_values = data.isnull().mean() * 100\n",
    "    missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "    st.subheader('Missing Values')\n",
    "    st.write(missing_values)\n",
    "\n",
    "    # Impute missing values using KNN imputer\n",
    "    if missing_values.sum() > 0:\n",
    "        imputer = KNNImputer(n_neighbors=5, weights='uniform', missing_values=np.nan)\n",
    "        data_imputed = imputer.fit_transform(data)\n",
    "        data = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "    st.write(f'Total missing values after imputation: {data.isnull().sum().sum()}')\n",
    "\n",
    "    # Normalize the data\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Split the data into features and labels\n",
    "    X = data.drop('class', axis=1)\n",
    "    y = data['class']\n",
    "    \n",
    "    # Ensure the target variable is discrete\n",
    "    if not np.issubdtype(y.dtype, np.integer):\n",
    "        y = y.astype(int)\n",
    "\n",
    "    # Correlation Analysis\n",
    "    st.subheader('Correlation Matrix')\n",
    "    corr_matrix = data.corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5, ax=ax)\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Mutual Information\n",
    "    st.subheader('Mutual Information')\n",
    "    numerical_features = data.drop(columns=['Genotype', 'Treatment', 'Behavior', 'class'])\n",
    "    target = data['class']\n",
    "\n",
    "    # Ensure the target variable is discrete\n",
    "    if not np.issubdtype(target.dtype, np.integer):\n",
    "        target = target.astype(int)\n",
    "\n",
    "    mi = mutual_info_classif(numerical_features, target, discrete_features=False)\n",
    "    mi_df = pd.DataFrame({'Feature': numerical_features.columns, 'Mutual Information': mi})\n",
    "    mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.barplot(x='Mutual Information', y='Feature', data=mi_df, ax=ax)\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Feature Importance from RandomForest\n",
    "    st.subheader('Feature Importance from RandomForest')\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(numerical_features, target)\n",
    "    importances = rf_model.feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': numerical_features.columns, 'Importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, ax=ax)\n",
    "    st.pyplot(fig)\n",
    "\n",
    "else:\n",
    "    st.write(\"Please upload a CSV file to proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd105961",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run dataanalysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a7499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
